{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Modeling\n",
    "The first part of this notebook follows closely the introductory examples in the Statsmodels online documentation.\n",
    "\n",
    "We won't try to provide the theoretical basis for regression models here.  You can consult any number of online resources for this, including this explanation of Ordinary Least Squares Regression in wikipedia: https://en.wikipedia.org/wiki/Ordinary_least_squares\n",
    "\n",
    "We will be using the Statsmodels library for this. Documentation is available here: http://statsmodels.sourceforge.net/stable/index.html\n",
    "\n",
    "The statistical model is assumed to be $Y = X\\beta + \\epsilon$, where $\\epsilon\\sim N\\left(0,\\sigma^{2}\\right)$\n",
    "\n",
    "We focus here on the simple Ordinary Least Squares (OLS) model that is the most widely used, but makes strong assumptions about the errors being indepentently and identically distributed (i.i.d.).  When these conditions are met, the OLS parameter estimates are the Best Linear Unbiased Estimates (BLUE).\n",
    "\n",
    "More intuitively (perhaps), what linear regression using the OLS estimator attempts to do is find the vector of parameters $\\beta$ such that when you compute a linear function $X\\beta$ you generate a predicted array of $\\hat{y}$ that, compared to the observed $y$, the squared sum of each observation's error $\\epsilon_{i} = \\hat{y}_{i} - y_{i}$ across all the observations $\\Sigma^{2}\\epsilon_{i}$, is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "\n",
    "\n",
    "# Generate artificial data (2 regressors + constant)\n",
    "nobs = 100\n",
    "X = np.random.random((nobs, 1))\n",
    "X = sm.add_constant(X)\n",
    "beta = [1.0, 3.5]\n",
    "e = np.random.normal(size=nobs)\n",
    "y_true = np.dot(X, beta)\n",
    "y = np.dot(X, beta) + e\n",
    "\n",
    "# Fit regression model\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Inspect the results\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the data graphically to better understand it.  First a scatterplot of our X variable against Y.  We skip the Intercept to get the actual X values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:,1], y, s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can manually add a plot of the true equation, without the error, and superimpose it.  What OLS is doing is minimizing the sum of all the errors -- the distance from each point to the line, across all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], y, color = 'r', s = 1)\n",
    "plt.plot(X[:,1], (beta[0] + beta[1] * X[:,1]), color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters: ', results.params)\n",
    "print('Standard errors: ', results.bse)\n",
    "print('Predicted values: ', results.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters: ', results.params)\n",
    "print('R2: ', results.rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prstd, iv_l, iv_u = wls_prediction_std(results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.plot(X[:,1], y, 'o', label=\"data\")\n",
    "ax.plot(X[:,1], y_true, 'b', label=\"True\")\n",
    "ax.plot(X[:,1], results.fittedvalues, 'r', label=\"OLS\")\n",
    "ax.plot(X[:,1], iv_u, 'r')\n",
    "ax.plot(X[:,1], iv_l, 'r')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lottery Example\n",
    "\n",
    "We next use a sample dataset that comes with statsmodels. $y$ is an $N \\times 1$ column of data on lottery wagers per capita (Lottery). $X$ is $N \\times 7$ with an intercept, the Literacy and Wealth variables, and 4 region binary variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas\n",
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we read a standard dataset that comes with Statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need all the columns, so let's create a reduced dataframe with only a few colunms in it.\n",
    "vars = ['Department', 'Lottery', 'Literacy', 'Wealth', 'Region']\n",
    "df = df[vars]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that Region is missing for the last row of data. Let's drop missing values from the dataframe.\n",
    "df = df.dropna()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The traditional way to specify the data for statsmodels regressions is using dmatrices,\n",
    "# that describe the vector of the dependent variable (y) and the 2D array of independent variables (X)\n",
    "# Note that this assigns the column to the left of the ~ to y, and the rest, to X\n",
    "y, X = dmatrices('Lottery ~ Literacy + Wealth + Region', data=df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice what happened to the Region variable in the X dmatrix. It automatically created dummy variables\n",
    "# from a categorical variable, since you cannot use categorical variables directly in a linear regression\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.OLS(y, X)    # Describe model. This creates a model object but produces no output.\n",
    "res = mod.fit()       # Fit model. This performs the statistical calculations, but does not output anything.\n",
    "print(res.summary())   # Summarize model. This prints the already computed model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice what it did with the Region variable that initially had 5 unique category values\n",
    "df['Region'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative way to specify models, using R syntax.  This uses the patsy language\n",
    "See http://patsy.readthedocs.org/en/latest/ for complete documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula='Lottery ~ Literacy + Wealth + Region', data=df)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Interaction Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula='Lottery ~ Literacy : Wealth + Region', data=df)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula='Lottery ~ Literacy * Wealth + Region', data=df)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now on to Hedonic Regression on Bay Area Housing Sales\n",
    "We will use a large sample of single family housing sales from the San Francisco Bay Area to estimate a linear regression model in which the dependent variable $y$ is the price of a house at the time of sale, and $X$ is a set of exogenous, or explanatory variables.\n",
    "\n",
    "What exactly does this give us?  A statistical way to figure out what the component amenities in a house are worth, if you could buy them *a la carte*.  Another way to think of it is, how much do house buyers in the Bay Area during this period pay, on average, for an additional unit of each amenity: square foot of living space, bedroom, bathroom, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load a sales transactions file. It has over 200K sales transactions in the Bay Area\n",
    "import pandas as pd, numpy as np\n",
    "sales = pd.read_csv('Data/homesales.csv')\n",
    "sales.rename(columns={ 'SQft' : 'sqft', 'Year_built': 'yrbuilt', 'Lot_size': 'lotsize', 'Sale_price': 'price'}, inplace=True)\n",
    "\n",
    "\n",
    "print(sales.columns)\n",
    "sales.drop(['RecordID', 'X', 'Y', 'sales', 'Sale_price_flt', 'parcel_id', '_node_id0', '_node_id1', '_node_id2',], axis=1, inplace=True)\n",
    "print(sales.columns)\n",
    "sales.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's load a set of walkscore-like calculations we computed and saved associated with each local street node\n",
    "# It contains a count of how many of each type of amenity are within a half-kilometer from the nearest node\n",
    "amenities = pd.read_csv('Data/amenityvalueindicators.csv')\n",
    "amenities.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now merge these two so we have local neighborhood amenities around each house that sold\n",
    "merged = pd.merge(sales, amenities, left_on='_node_id', right_on='node_id')\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some specific key variables\n",
    "%matplotlib inline\n",
    "merged['price'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged.plot(kind='scatter',x='sqft',y='price', s=1)\n",
    "plt.figure(1, figsize=(10,8), )\n",
    "plt.ylim(0,3000000)\n",
    "plt.xlim(0, 5000)\n",
    "plt.scatter(merged['sqft'], merged['price'], s=.01, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cleaned.quantile(.01))\n",
    "#print(cleaned.quantile(.99))\n",
    "minprice = merged['price'].quantile(.01)\n",
    "maxprice = merged['price'].quantile(.99)\n",
    "minsqft = merged['sqft'].quantile(.01)\n",
    "maxsqft = merged['sqft'].quantile(.99)\n",
    "minlot = merged['lotsize'].quantile(.01)\n",
    "maxlot = merged['lotsize'].quantile(.9)\n",
    "minyr = 1900\n",
    "maxyr = 2016\n",
    "\n",
    "cleaned = merged[(merged['price'] > minprice) & (merged['price'] < maxprice) \n",
    "                & (merged['sqft'] > minsqft) & (merged['sqft'] < maxsqft)\n",
    "                & (merged['yrbuilt'] > 0) & (merged['yrbuilt'] < 2016)\n",
    "                & (merged['yrbuilt'] > minyr) & (merged['yrbuilt'] < maxyr)\n",
    "                & (merged['lotsize'] > minlot) & (merged['lotsize'] < maxlot)\n",
    "                ]\n",
    "\n",
    "cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['price'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['sqft'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['lotsize'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much does price correlate with square footage of the house?\n",
    "#cleaned.plot(kind='scatter',x='sqft',y='price', s=.01, color='g')\n",
    "plt.figure(1, figsize=(10,8), )\n",
    "plt.ylim(0,3000000)\n",
    "plt.xlim(0, 5000)\n",
    "plt.scatter(cleaned['sqft'], cleaned['price'], s=.01, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much does price vary with Lot size?\n",
    "plt.figure(1, figsize=(10,8), )\n",
    "plt.ylim(0,3000000)\n",
    "plt.xlim(0, 10000)\n",
    "plt.scatter(cleaned['lotsize'], cleaned['price'], s=.01, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's estimate some hedonic regression models, starting simple and adding variables\n",
    "# We start with price regressed on sqft and an intercept\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "results = smf.ols('price ~  sqft', data=cleaned).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try interpreting the coefficients of the model above.  What do they mean?\n",
    "\n",
    "Next we try taking log transformations of the dependent and independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hedonic regressions are often log-transformed, as semi-log or log-log, with dependent variable log-transformed,\n",
    "# and some independent variables depending on the data, to improve model fit\n",
    "results = smf.ols('np.log(price) ~  np.log(sqft)', data=cleaned).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add lotsize\n",
    "results = smf.ols('np.log(price) ~  np.log(sqft) + np.log(lotsize)', data=cleaned).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a dummy variable or houses built before 1940\n",
    "results = smf.ols('np.log(price) ~  np.log(sqft) + np.log(lotsize) + yrbuilt<1940',\n",
    "                 data=cleaned).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin exploring how accessibility influences prices\n",
    "results = smf.ols('np.log(price) ~  np.log(sqft) + np.log(lotsize) + yrbuilt<1940 + shopping',\n",
    "                 data=cleaned).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned.loc[:, 'sanfran'] = cleaned['City']=='San Francisco'\n",
    "cleaned['sanfran'] = False\n",
    "cleaned['sanfran'] = cleaned['City']=='San Francisco'\n",
    "results = smf.ols('np.log(price) ~  np.log(sqft) + np.log(lotsize) + yrbuilt<1940 + shopping + sanfran', \n",
    "                 data=cleaned).fit()\n",
    "print(results.summary())\n",
    "print('Number of San Francisco sales: ', cleaned['sanfran'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors =  (results.predict() - np.log(cleaned['price'])) / np.log(cleaned['price'])\n",
    "errors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,8), )\n",
    "plt.ylim(11.5, 14.5)\n",
    "plt.xlim(11.5, 14.5)\n",
    "plt.scatter(np.log(cleaned['price']), results.predict(), s=.01, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,8), )\n",
    "plt.scatter(np.log(cleaned['price']), errors, s=.01, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_low = cleaned[cleaned['price'] < 600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_low = smf.ols('np.log(price) ~  np.log(sqft) + np.log(lotsize) + yrbuilt<1940 + shopping + sanfran', \n",
    "                 data=cleaned_low).fit()\n",
    "print(results_low.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_high = cleaned[cleaned['price'] > 600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_high = smf.ols('np.log(price) ~  np.log(sqft) + np.log(lotsize) + yrbuilt<1940 + shopping + sanfran', \n",
    "                 data=cleaned_high).fit()\n",
    "print(results_high.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lin = smf.ols('price ~  sqft + lotsize + yrbuilt<1940 + shopping + sanfran', \n",
    "                 data=cleaned).fit()\n",
    "print(results_lin.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,8), )\n",
    "plt.ylim(200000, 2000000)\n",
    "plt.xlim(200000, 2000000)\n",
    "plt.scatter(cleaned['price'], results_lin.predict(), s=.01, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Below is an experiment with Robust Linear Models in Statsmodels\n",
    "\n",
    "See the docs for more background on this and related approaches:\n",
    "http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/robust_models_0.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = dmatrices('price ~  sqft + lotsize + yrbuilt<1940 + shopping + sanfran', data=cleaned, return_type='dataframe')\n",
    "huber_t = sm.RLM(y, X, M=sm.robust.norms.HuberT())\n",
    "hub_results = huber_t.fit()\n",
    "print('Parameters \\n')\n",
    "print(hub_results.params)\n",
    "print()\n",
    "print('Standard Errors \\n')\n",
    "print(hub_results.bse)\n",
    "print(hub_results.summary(yname='y',\n",
    "            xname=['var_%d' % i for i in range(len(hub_results.params))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "* Experiment with the regression model to see how much you can improve this regression model and its fit to the data.  \n",
    "\n",
    "* If you want to learn more about robust regression, experiment with it and try to compare its prediction errors with that of OLS for rhe same model specification.\n",
    "\n",
    "* Try loading the rentals listing data and census data we used in the last session, merge them, and set up a hedonic regression on rents in the Bay Area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
